{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "25TB5xWC-BBd",
      "metadata": {
        "id": "25TB5xWC-BBd"
      },
      "source": [
        "# Team              : CTRL C CTRL V\n",
        "# Track Company     : Track-1 Cactus Communications\n",
        "# Track Name        : Paperpal - Future of Academic Writing\n",
        "# Chec Type         : Content Centric Checks\n",
        "# Check Implemented : Publication Integrity\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b521204b",
      "metadata": {},
      "source": [
        "### Salient Features: \n",
        "\n",
        "1. Typo errors are flagged and checked, by converting a pdf and preprocessing data using series of  preprocessors, autocorrects and spell checkers.\n",
        "2. Plagiarism Checker uses web scraping from Wikipedia, assumed to be the authentic source, converted to a corpus, passed through a series of preprocessors and uses state-of-the art doc2Vec model. The model is trained and evaluated for, against our pdf/docx, uses co-sine similarity and produces results in decreasing sorted order of similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9308a1db",
      "metadata": {},
      "source": [
        "# Typo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "LGjHnAhZG5XW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGjHnAhZG5XW",
        "outputId": "f4bec0eb-2e5b-4d44-d45a-24229d6c81e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.8/dist-packages (3.0.1)\n",
            "Requirement already satisfied: typing_extensions>=3.10.0.0 in /usr/local/lib/python3.8/dist-packages (from PyPDF2) (4.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "R5v7gHYSRMsb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5v7gHYSRMsb",
        "outputId": "1ec2c5ba-d792-4cd5-c990-e3513fc7b583"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "iMUySjknGw8Q",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMUySjknGw8Q",
        "outputId": "8ea1394a-0172-4f53-afcc-d96ce7595faf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            "978-1-5386-1010-7/17/$31.00 ©2017 IEEE                          97  MALICIOUS URL DETECTION USING MULTI-LAYER FILTERING MODEL  \n",
            "RAJESH KUMAR, XIAOSONG ZHANG, HUSSAIN AHMAD TARIQ, RIAZ ULLAH KHAN \n",
            "School of Computer Science & Engineering, University of Electro nic Science and Technology of China  \n",
            "E-MAIL: rajakumarlohano@gmail.com, rerukhan@outlook.com\n",
            "Abstract: \n",
            "Malicious URLs are harmful to every aspect of computer \n",
            "users. Detecting of the malicious URL is very important.  \n",
            "Currently, detection of malicious webpages techniques \n",
            "includes  black-list and white-list  methodology and machine \n",
            "learning  classification algorithms are used. However, the \n",
            "black-list and  white-list technology is useless if a particular \n",
            "URL is not in  list. In this paper, we propose a multi-layer \n",
            "model for detecting  malicious  URL. The filter can directly \n",
            "determine the URL by  training the threshold of each layer \n",
            "filter when it reaches  the threshold. Otherwise, the filter \n",
            "l e a v e s  t h e  U R L  t o  n e x t  l a y e r .  W e  also used an example to \n",
            "verify that  the model can improve the  accuracy of URL \n",
            "detection. \n",
            "Keywords: \n",
            "Malicious URL; Black-list and White-list Tech nology ; \n",
            "Machine Learning; Multi-layer Filtering Model  \n",
            "1. Introduction \n",
            "In recent years, the Internet has been playing a bigger \n",
            "and bigger role in people’s work and life. Currently, we \n",
            "observed  that not every website is user-friendly and \n",
            "profitable. More and  more malicious websites began to \n",
            "appear and these malicious  websites endangering all \n",
            "aspects of the user. This can lead  the user towards \n",
            "economic losses, even some can create  confusion over the \n",
            "management of the country. Detecting and  stopping \n",
            "malicious websites has become an important control  \n",
            "measure to avoid the risk of information security [7-8] [10]. \n",
            "Generally, the only entrance to the website is URL, which \n",
            "can be malicious URL. At this level of entrance, the \n",
            "identification  o f  U R L  i s  t h e  b e s t  s o l u t i o n  t o  a v o i d  \n",
            "information loss. So the  malicious URL identification has \n",
            "a l w a y s  b e e n  a  h o t  a r e a  o f  information security. \n",
            "Spams, malicious webpages and URLs that redirect or  \n",
            "mislead the legitimate  users to malware, scams, or adult  \n",
            "content. It is perhaps correlated with the use of the internet.  \n",
            "To identify  malicious URLs, ML-based classifiers draw \n",
            "features  from webpages content (lexical, visual, etc.), URL \n",
            "lexical  features, redirect paths, host -based features, or some combinations of them. Such classifiers usually act in \n",
            "conjunction  with knowledge bases which are usually  in-\n",
            "browser URL BLs  or from web service providers. If the \n",
            "classifier is fed with  URL-based features, it is c o m m o n  \n",
            "t o  s e t  a  U R L  a g g r e g a t o r  a s  a  p e r - p r o c e s s o r  b e f o r e  \n",
            "extracting features. Mostly using  supervised learning \n",
            "paradigm, Naive  Bayes [1][13], Support V ector  Machine \n",
            "with different kernels [9], and Logistic Regression are  \n",
            "popular Machine Learning classifiers for filtering spam \n",
            "and phishing [6]. Meanwhile, GT-based learning to deal with \n",
            "active  attackers is also evaluated in  spam filtering. In this \n",
            "paper we solve this problem using multi-layer filtering \n",
            "model to do each classification which are able to handle their own relatively good data.  \n",
            "2. Multi-Layer Filter Model \n",
            "Malicious URL detection [5]\n",
            "[11-12] is a typical \n",
            "classification application scenario. The URL may be malicious URL\n",
            " o r  n o r m a l  U R L .  T h e  f i r s t  part o f  s e v e r a l  \n",
            "machine learning  classification algorithms are very useful. \n",
            "A wide range of applications have also been applied to \n",
            "malicious URL detection  scenarios. In order to give a brief \n",
            "overview to the advantages  of each classifier, this article  \n",
            "designed a malicious URL Multi-  layer detection model. \n",
            "This model is mainly composed of 4-  layer classifier, where \n",
            "these  classifiers are also called filters in  this model, so the \n",
            "model consists of 4 Layer filter composition.  \n",
            "2.1. Stratified filter \n",
            "1) Black and white list filter: The model’s first-level \n",
            "filter  is a black-and-white list filter which will be validated \n",
            "by recognizing the normal URLs and Malicious URLs. The  \n",
            "normal URL addresses are stored in the white list file, \n",
            "while  t h e  m a l i c i o u s  U R L  a d d r e s s e s  a r e  s t o r e d  i n  t h e  \n",
            "blacklist file.  To detect  the URL, we traverse the list of \n",
            "black and white to  determine whether the URL in the black \n",
            "list or in the white  list. \n",
            "2) Naive Bayesian filter: The second layer filter in \n",
            "this model is a naive Bayesian  filter that trains the model by  \n",
            "Authorized licensed use limited to: Nirma University Institute of Technology. Downloaded on September 04,2022 at 13:15:59 UTC from IEEE Xplore.  Restrictions apply. \n",
            " \n",
            "978-1-5386-1010-7/17/$31.00 ©2017 IEEE                          97  MALICIOUS URL DETECTION USING MULTI-LAYER FILTERING MODEL  \n",
            "RAJESH KUMAR, XIAOSONG ZHANG, HUSSAIN AHMAD TARIQ, RIAZ ULLAH KHAN \n",
            "School of Computer Science & Engineering, University of Electro nic Science and Technology of China  \n",
            "E-MAIL: rajakumarlohano@gmail.com, rerukhan@outlook.com\n",
            "Abstract: \n",
            "Malicious URLs are harmful to every aspect of computer \n",
            "users. Detecting of the malicious URL is very important.  \n",
            "Currently, detection of malicious webpages techniques \n",
            "includes  black-list and white-list  methodology and machine \n",
            "learning  classification algorithms are used. However, the \n",
            "black-list and  white-list technology is useless if a particular \n",
            "URL is not in  list. In this paper, we propose a multi-layer \n",
            "model for detecting  malicious  URL. The filter can directly \n",
            "determine the URL by  training the threshold of each layer \n",
            "filter when it reaches  the threshold. Otherwise, the filter \n",
            "l e a v e s  t h e  U R L  t o  n e x t  l a y e r .  W e  also used an example to \n",
            "verify that  the model can improve the  accuracy of URL \n",
            "detection. \n",
            "Keywords: \n",
            "Malicious URL; Black-list and White-list Tech nology ; \n",
            "Machine Learning; Multi-layer Filtering Model  \n",
            "1. Introduction \n",
            "In recent years, the Internet has been playing a bigger \n",
            "and bigger role in people’s work and life. Currently, we \n",
            "observed  that not every website is user-friendly and \n",
            "profitable. More and  more malicious websites began to \n",
            "appear and these malicious  websites endangering all \n",
            "aspects of the user. This can lead  the user towards \n",
            "economic losses, even some can create  confusion over the \n",
            "management of the country. Detecting and  stopping \n",
            "malicious websites has become an important control  \n",
            "measure to avoid the risk of information security [7-8] [10]. \n",
            "Generally, the only entrance to the website is URL, which \n",
            "can be malicious URL. At this level of entrance, the \n",
            "identification  o f  U R L  i s  t h e  b e s t  s o l u t i o n  t o  a v o i d  \n",
            "information loss. So the  malicious URL identification has \n",
            "a l w a y s  b e e n  a  h o t  a r e a  o f  information security. \n",
            "Spams, malicious webpages and URLs that redirect or  \n",
            "mislead the legitimate  users to malware, scams, or adult  \n",
            "content. It is perhaps correlated with the use of the internet.  \n",
            "To identify  malicious URLs, ML-based classifiers draw \n",
            "features  from webpages content (lexical, visual, etc.), URL \n",
            "lexical  features, redirect paths, host -based features, or some combinations of them. Such classifiers usually act in \n",
            "conjunction  with knowledge bases which are usually  in-\n",
            "browser URL BLs  or from web service providers. If the \n",
            "classifier is fed with  URL-based features, it is c o m m o n  \n",
            "t o  s e t  a  U R L  a g g r e g a t o r  a s  a  p e r - p r o c e s s o r  b e f o r e  \n",
            "extracting features. Mostly using  supervised learning \n",
            "paradigm, Naive  Bayes [1][13], Support V ector  Machine \n",
            "with different kernels [9], and Logistic Regression are  \n",
            "popular Machine Learning classifiers for filtering spam \n",
            "and phishing [6]. Meanwhile, GT-based learning to deal with \n",
            "active  attackers is also evaluated in  spam filtering. In this \n",
            "paper we solve this problem using multi-layer filtering \n",
            "model to do each classification which are able to handle their own relatively good data.  \n",
            "2. Multi-Layer Filter Model \n",
            "Malicious URL detection [5]\n",
            "[11-12] is a typical \n",
            "classification application scenario. The URL may be malicious URL\n",
            " o r  n o r m a l  U R L .  T h e  f i r s t  part o f  s e v e r a l  \n",
            "machine learning  classification algorithms are very useful. \n",
            "A wide range of applications have also been applied to \n",
            "malicious URL detection  scenarios. In order to give a brief \n",
            "overview to the advantages  of each classifier, this article  \n",
            "designed a malicious URL Multi-  layer detection model. \n",
            "This model is mainly composed of 4-  layer classifier, where \n",
            "these  classifiers are also called filters in  this model, so the \n",
            "model consists of 4 Layer filter composition.  \n",
            "2.1. Stratified filter \n",
            "1) Black and white list filter: The model’s first-level \n",
            "filter  is a black-and-white list filter which will be validated \n",
            "by recognizing the normal URLs and Malicious URLs. The  \n",
            "normal URL addresses are stored in the white list file, \n",
            "while  t h e  m a l i c i o u s  U R L  a d d r e s s e s  a r e  s t o r e d  i n  t h e  \n",
            "blacklist file.  To detect  the URL, we traverse the list of \n",
            "black and white to  determine whether the URL in the black \n",
            "list or in the white  list. \n",
            "2) Naive Bayesian filter: The second layer filter in \n",
            "this model is a naive Bayesian  filter that trains the model by  \n",
            "Authorized licensed use limited to: Nirma University Institute of Technology. Downloaded on September 04,2022 at 13:15:59 UTC from IEEE Xplore.  Restrictions apply.  \n",
            "98 dividing into two main steps:  \n",
            "By training the URL samples, we use two-dimensional \n",
            "arrays C1 and C2 to store the probability of each value \n",
            "of malicious website and normal website, such as formula \n",
            "(1) below:  \n",
            "11 12 1\n",
            "12, , ...\n",
            "c\n",
            ",. . .m\n",
            "i\n",
            "nn n mPP P\n",
            "PP P§·\n",
            "¨¸ ¨¸¨¸©¹\n",
            "¸\n",
            "¸\n",
            "¸\n",
            "¸\n",
            "¸\n",
            "¸\n",
            "¸         ( 1 )   \n",
            "2.2. Alpha N-Bayes threshold training \n",
            "Let m a x ( p 1/p2 ,p2/p 1 )nbayesD   (where P1 \n",
            "and P2 represent respectively what is calculated by the naive \n",
            "Bayesian formula model as malicious URL and normal URL \n",
            "probability value), so the size of nbayes can represent this classification judgment of the credibility. The nbayes describes that the URL belongs\n",
            " to one of the categories. The \n",
            "probability is much greater than the probability of belonging to another class. So, when nbayes arrives at certain threshold \n",
            "size, we can consider that URL is Naive Bayesian good data. \n",
            "If this threshold is set to too small, it may not be reached to  \n",
            "the ideal classification accuracy. If this threshold is set to too \n",
            "large, naive shellfish the data that yeast good filters will be  \n",
            "very small. This article discusses another group training data to train this threshold. The specific method is discussed \n",
            "briefly in Section 3  \n",
            "Assume that the appropriate training threshold isnbayesD . \n",
            "For the upper filter down to detect URL, put it into the traine d \n",
            "naive Bayesian model, and calculate nbayes if\n",
            "nbayes nbayesDD! , then we think the URL is a good Bayesian \n",
            "model of good data, otherwise, the URL cannot be considered as a good data for a naive Bayesian model, i.e. it cannot determine the nature of the URL, so record the classification results, and move to the next filte\n",
            "r. \n",
            "2.3. CART decision tree filter \n",
            "The model’s third-level filter is CART Decision Tree \n",
            "Filter.  \n",
            "The model is further split into two steps:  \n",
            "1) model training: The CART tree is constructed by \n",
            "training samples with URLs, and the leaf nodes are decision nodes, and store the CART tree in the file system.  \n",
            "2) cart threshold training: \n",
            "Let\n",
            "max(n/ m, m/ n)nbayesD   (n represents the number of \n",
            "malicious URL leaf nodes, m represents the number of \n",
            "normal URL leaf nodes), so the size of a cart can characterize the type of occupation that a leaf node decides. Similar to \n",
            "naive Bayesian filter, this threshold can neither be set too small nor too large. So, to train the threshold, through the \n",
            "same training data  group, the specific method is discussed in \n",
            "Section 3.  \n",
            "Ifnbayes nbayesDD! , then we think that URL is CART \n",
            "Decision tree model has a good at data, otherwise, record the classification results, and the URL is filtered to the next fil ter. \n",
            "2.4. SVM filter \n",
            "The final filter of this model is SVM filter. SVM training \n",
            "model is mainly classified models. It Derived classification function for the upper Layer filter down the URL, record classification results, combined with Naive Bayesian Filter and CART Decision Tree filtering collectively determine the \n",
            "classification of the URL.  \n",
            "3. Instance Validation \n",
            "3.1. Data sources and experimental\n",
            " environment \u0003\u0003\n",
            "The   malicious   URL    dataset    in    \n",
            "this    experiment  is downloaded from the malicious \n",
            "website lab  (Http://www.mwsl.org.cn/), the normal  U R L  \n",
            "dataset is collected from first category directory \n",
            "(http://www.dir001.com/).  10000 samples are taken from \n",
            "each dataset i.e. 10000 from  malicious URLs and 10000 \n",
            "from normal URLs. This article  uses features extraction \n",
            "and data modeling. Python language  is u s e d  a s  t h e  \n",
            "implementation programming. Windows 10  64bit as an \n",
            "operating system and Core i5 with 16GB RAM  was used \n",
            "as personal computer.  \n",
            "Table 1 feature vector extraction rules  \n",
            "No Features  \n",
            "F1 The domain names contained more  than 4 consecutive numbers  \n",
            "F2 The domain name contains special  characters (#, $, @, ~, _, -)  \n",
            "F3 Top Five domain name (com, en, net,  org, cc)  \n",
            "F4 The number of “.” in domain name  \n",
            "F5 domain name total length  \n",
            "F6 The Length of longest domain name  segment  \n",
            " \n",
            "F7 Meaningful coefficients in primary  domain names  \n",
            "3.2. Feature Selection \n",
            "Garera[2] and Gattani[3] make comparisons of \n",
            "comprehensive study regarding URL feature selection. This article mainly from the perspective of domain name cost, malicious website The creator knows that the domain name of his site has a great risk of being banned, so in the purchas e \n",
            "Authorized licensed use limited to: Nirma University Institute of Technology. Downloaded on September 04,2022 at 13:15:59 UTC from IEEE Xplore.  Restrictions apply. \n",
            " \n",
            "978-1-5386-1010-7/17/$31.00 ©2017 IEEE                          97  MALICIOUS URL DETECTION USING MULTI-LAYER FILTERING MODEL  \n",
            "RAJESH KUMAR, XIAOSONG ZHANG, HUSSAIN AHMAD TARIQ, RIAZ ULLAH KHAN \n",
            "School of Computer Science & Engineering, University of Electro nic Science and Technology of China  \n",
            "E-MAIL: rajakumarlohano@gmail.com, rerukhan@outlook.com\n",
            "Abstract: \n",
            "Malicious URLs are harmful to every aspect of computer \n",
            "users. Detecting of the malicious URL is very important.  \n",
            "Currently, detection of malicious webpages techniques \n",
            "includes  black-list and white-list  methodology and machine \n",
            "learning  classification algorithms are used. However, the \n",
            "black-list and  white-list technology is useless if a particular \n",
            "URL is not in  list. In this paper, we propose a multi-layer \n",
            "model for detecting  malicious  URL. The filter can directly \n",
            "determine the URL by  training the threshold of each layer \n",
            "filter when it reaches  the threshold. Otherwise, the filter \n",
            "l e a v e s  t h e  U R L  t o  n e x t  l a y e r .  W e  also used an example to \n",
            "verify that  the model can improve the  accuracy of URL \n",
            "detection. \n",
            "Keywords: \n",
            "Malicious URL; Black-list and White-list Tech nology ; \n",
            "Machine Learning; Multi-layer Filtering Model  \n",
            "1. Introduction \n",
            "In recent years, the Internet has been playing a bigger \n",
            "and bigger role in people’s work and life. Currently, we \n",
            "observed  that not every website is user-friendly and \n",
            "profitable. More and  more malicious websites began to \n",
            "appear and these malicious  websites endangering all \n",
            "aspects of the user. This can lead  the user towards \n",
            "economic losses, even some can create  confusion over the \n",
            "management of the country. Detecting and  stopping \n",
            "malicious websites has become an important control  \n",
            "measure to avoid the risk of information security [7-8] [10]. \n",
            "Generally, the only entrance to the website is URL, which \n",
            "can be malicious URL. At this level of entrance, the \n",
            "identification  o f  U R L  i s  t h e  b e s t  s o l u t i o n  t o  a v o i d  \n",
            "information loss. So the  malicious URL identification has \n",
            "a l w a y s  b e e n  a  h o t  a r e a  o f  information security. \n",
            "Spams, malicious webpages and URLs that redirect or  \n",
            "mislead the legitimate  users to malware, scams, or adult  \n",
            "content. It is perhaps correlated with the use of the internet.  \n",
            "To identify  malicious URLs, ML-based classifiers draw \n",
            "features  from webpages content (lexical, visual, etc.), URL \n",
            "lexical  features, redirect paths, host -based features, or some combinations of them. Such classifiers usually act in \n",
            "conjunction  with knowledge bases which are usually  in-\n",
            "browser URL BLs  or from web service providers. If the \n",
            "classifier is fed with  URL-based features, it is c o m m o n  \n",
            "t o  s e t  a  U R L  a g g r e g a t o r  a s  a  p e r - p r o c e s s o r  b e f o r e  \n",
            "extracting features. Mostly using  supervised learning \n",
            "paradigm, Naive  Bayes [1][13], Support V ector  Machine \n",
            "with different kernels [9], and Logistic Regression are  \n",
            "popular Machine Learning classifiers for filtering spam \n",
            "and phishing [6]. Meanwhile, GT-based learning to deal with \n",
            "active  attackers is also evaluated in  spam filtering. In this \n",
            "paper we solve this problem using multi-layer filtering \n",
            "model to do each classification which are able to handle their own relatively good data.  \n",
            "2. Multi-Layer Filter Model \n",
            "Malicious URL detection [5]\n",
            "[11-12] is a typical \n",
            "classification application scenario. The URL may be malicious URL\n",
            " o r  n o r m a l  U R L .  T h e  f i r s t  part o f  s e v e r a l  \n",
            "machine learning  classification algorithms are very useful. \n",
            "A wide range of applications have also been applied to \n",
            "malicious URL detection  scenarios. In order to give a brief \n",
            "overview to the advantages  of each classifier, this article  \n",
            "designed a malicious URL Multi-  layer detection model. \n",
            "This model is mainly composed of 4-  layer classifier, where \n",
            "these  classifiers are also called filters in  this model, so the \n",
            "model consists of 4 Layer filter composition.  \n",
            "2.1. Stratified filter \n",
            "1) Black and white list filter: The model’s first-level \n",
            "filter  is a black-and-white list filter which will be validated \n",
            "by recognizing the normal URLs and Malicious URLs. The  \n",
            "normal URL addresses are stored in the white list file, \n",
            "while  t h e  m a l i c i o u s  U R L  a d d r e s s e s  a r e  s t o r e d  i n  t h e  \n",
            "blacklist file.  To detect  the URL, we traverse the list of \n",
            "black and white to  determine whether the URL in the black \n",
            "list or in the white  list. \n",
            "2) Naive Bayesian filter: The second layer filter in \n",
            "this model is a naive Bayesian  filter that trains the model by  \n",
            "Authorized licensed use limited to: Nirma University Institute of Technology. Downloaded on September 04,2022 at 13:15:59 UTC from IEEE Xplore.  Restrictions apply.  \n",
            "98 dividing into two main steps:  \n",
            "By training the URL samples, we use two-dimensional \n",
            "arrays C1 and C2 to store the probability of each value \n",
            "of malicious website and normal website, such as formula \n",
            "(1) below:  \n",
            "11 12 1\n",
            "12, , ...\n",
            "c\n",
            ",. . .m\n",
            "i\n",
            "nn n mPP P\n",
            "PP P§·\n",
            "¨¸ ¨¸¨¸©¹\n",
            "¸\n",
            "¸\n",
            "¸\n",
            "¸\n",
            "¸\n",
            "¸\n",
            "¸         ( 1 )   \n",
            "2.2. Alpha N-Bayes threshold training \n",
            "Let m a x ( p 1/p2 ,p2/p 1 )nbayesD   (where P1 \n",
            "and P2 represent respectively what is calculated by the naive \n",
            "Bayesian formula model as malicious URL and normal URL \n",
            "probability value), so the size of nbayes can represent this classification judgment of the credibility. The nbayes describes that the URL belongs\n",
            " to one of the categories. The \n",
            "probability is much greater than the probability of belonging to another class. So, when nbayes arrives at certain threshold \n",
            "size, we can consider that URL is Naive Bayesian good data. \n",
            "If this threshold is set to too small, it may not be reached to  \n",
            "the ideal classification accuracy. If this threshold is set to too \n",
            "large, naive shellfish the data that yeast good filters will be  \n",
            "very small. This article discusses another group training data to train this threshold. The specific method is discussed \n",
            "briefly in Section 3  \n",
            "Assume that the appropriate training threshold isnbayesD . \n",
            "For the upper filter down to detect URL, put it into the traine d \n",
            "naive Bayesian model, and calculate nbayes if\n",
            "nbayes nbayesDD! , then we think the URL is a good Bayesian \n",
            "model of good data, otherwise, the URL cannot be considered as a good data for a naive Bayesian model, i.e. it cannot determine the nature of the URL, so record the classification results, and move to the next filte\n",
            "r. \n",
            "2.3. CART decision tree filter \n",
            "The model’s third-level filter is CART Decision Tree \n",
            "Filter.  \n",
            "The model is further split into two steps:  \n",
            "1) model training: The CART tree is constructed by \n",
            "training samples with URLs, and the leaf nodes are decision nodes, and store the CART tree in the file system.  \n",
            "2) cart threshold training: \n",
            "Let\n",
            "max(n/ m, m/ n)nbayesD   (n represents the number of \n",
            "malicious URL leaf nodes, m represents the number of \n",
            "normal URL leaf nodes), so the size of a cart can characterize the type of occupation that a leaf node decides. Similar to \n",
            "naive Bayesian filter, this threshold can neither be set too small nor too large. So, to train the threshold, through the \n",
            "same training data  group, the specific method is discussed in \n",
            "Section 3.  \n",
            "Ifnbayes nbayesDD! , then we think that URL is CART \n",
            "Decision tree model has a good at data, otherwise, record the classification results, and the URL is filtered to the next fil ter. \n",
            "2.4. SVM filter \n",
            "The final filter of this model is SVM filter. SVM training \n",
            "model is mainly classified models. It Derived classification function for the upper Layer filter down the URL, record classification results, combined with Naive Bayesian Filter and CART Decision Tree filtering collectively determine the \n",
            "classification of the URL.  \n",
            "3. Instance Validation \n",
            "3.1. Data sources and experimental\n",
            " environment \u0003\u0003\n",
            "The   malicious   URL    dataset    in    \n",
            "this    experiment  is downloaded from the malicious \n",
            "website lab  (Http://www.mwsl.org.cn/), the normal  U R L  \n",
            "dataset is collected from first category directory \n",
            "(http://www.dir001.com/).  10000 samples are taken from \n",
            "each dataset i.e. 10000 from  malicious URLs and 10000 \n",
            "from normal URLs. This article  uses features extraction \n",
            "and data modeling. Python language  is u s e d  a s  t h e  \n",
            "implementation programming. Windows 10  64bit as an \n",
            "operating system and Core i5 with 16GB RAM  was used \n",
            "as personal computer.  \n",
            "Table 1 feature vector extraction rules  \n",
            "No Features  \n",
            "F1 The domain names contained more  than 4 consecutive numbers  \n",
            "F2 The domain name contains special  characters (#, $, @, ~, _, -)  \n",
            "F3 Top Five domain name (com, en, net,  org, cc)  \n",
            "F4 The number of “.” in domain name  \n",
            "F5 domain name total length  \n",
            "F6 The Length of longest domain name  segment  \n",
            " \n",
            "F7 Meaningful coefficients in primary  domain names  \n",
            "3.2. Feature Selection \n",
            "Garera[2] and Gattani[3] make comparisons of \n",
            "comprehensive study regarding URL feature selection. This article mainly from the perspective of domain name cost, malicious website The creator knows that the domain name of his site has a great risk of being banned, so in the purchas e \n",
            "Authorized licensed use limited to: Nirma University Institute of Technology. Downloaded on September 04,2022 at 13:15:59 UTC from IEEE Xplore.  Restrictions apply.  \n",
            "99 When buying a domain name, you often buy cheaper or even \n",
            "free domains for cost savings Name, and this domain often has the following characteristics: 1) TLD is not the mainstream Domain name; 2) domain name with special characters; 3) domain name length is very long; 4) The main \n",
            "domain consists of meaningless letters or numbers; 5) There \n",
            "are many \".\" To confuse the domain name structure.  \n",
            "Based on the above information, this paper chooses \n",
            "seven \n",
            "features, as shown in Table 1.  \n",
            "The calculation method of F7 in Table 1 is as follows:  \n",
            "This text chooses the commonly used 5492 English \n",
            "words and 187,207 Chinese word phonetic together \n",
            "constitute a meaningful word Tree, the string of the URL ÿs \n",
            "main domain name is a meaningful list of the tree of words \n",
            "to match, if the match can be considered match these characters are there Meaning, and then use the rules of the \n",
            "characters and the total length of the characters that have ratio Meaning coefficient. For example, bookdsxihuan.com, \n",
            "because the domain name does not contain 4.  \n",
            "More than one consecutive number, so F1 is set to 0; \n",
            "due to the domain name does not contain Any of the following special words: #, $, @, ~, \u0003-, _, so F2 is set to 0; \n",
            "This example contains one of the top five domain names \"com\", so F3 is set to 1; The domain name contains a \".\" \n",
            "Therefore F4 is set to 1; F5 is the total length of the domain \n",
            "name, This example is 16; in th is case, the longest domain \n",
            "name is \"bookdsxihuan\" and its length is 12, so F6 is set \u0003to \n",
            "12; the main domain name of this example is \u0003\n",
            "\"bookdsxihuan\" with two A meaningful word \"book\", \n",
            "\"xihuan\" match \u000f\u0003meaningful length of 10, The primary \n",
            "domain has a length of 12, then it has a meaningful \n",
            "coefficient of 10/12 = 0.83. From this, the feature vector of \n",
            "this domain name is expressed as {0,0,1,1,16,12,0.83}.  \n",
            "Table 2 url data set  \n",
            "Dataset Name  Dataset Description  \n",
            "Training model dataset  8000 malicious URL, 8000 normal URL  \n",
            "Training threshold \n",
            "dataset  1000 malicious URL, 1000 normal URL  \n",
            "Testing dataset  1000 malicious URL, 1000 normal URL  \n",
            "Table 3 test results for various models  \n",
            "Model Name Accuracy \n",
            "Rate % Rexall \n",
            "Rate % Precise \n",
            "rate % \n",
            "Multi-layer filtering model 79.55 68.80 87.64 \n",
            "Simple Naive Bayes 77.30 66.40 84.91 \n",
            "Single Decision Tree 79.35 69.00 87.01 \n",
            "Single SVM 76.80 79.40 75.48 \n",
            "3.3. Training model and threshold \n",
            "In this paper, we divide the 20,000 URLs collected in \n",
            "section 3.1 into three in the experiment Part, as shown in Table 2.  \n",
            "If the URL which is to be tested can directly be judged \n",
            "as malicious or normal, so the  experimental tuning process \n",
            "does not consider black and white list filter. The blacklist filter can be considered authoritative in this model. The \n",
            "experimental \n",
            "process is as follows:  \n",
            "Perform the data on the three datasets using the feature \n",
            "extraction rules in Section 3.2  \n",
            "Train a separate machine learning with the train-model-\n",
            "set sample set Classifiers, including the naive Bayesian \n",
            "model, the CART decision tree model and SVM model  \n",
            "Build a separate machine learning model into a \n",
            "multilayer filter model, and at the very beginning give a very \n",
            "large threshold pair, so 500, 10new new\n",
            "nbayes cart DD   \n",
            "Substitute the train- threshold -set sample set into the multi-\n",
            "layer filter model and calculate the check measurement \n",
            "accuracy, and gradually reduce these two thresholds, this \n",
            "article uses the formula in the program\n",
            "1 0.8 , 1new old new old\n",
            "nbayes nbayes cart cart DD D D \u000e  \u000e  , record each \n",
            "thresholds accuracy, and finally pick the combination of the \n",
            "highest accuracy of the function **(, )nbayes cartDD  \n",
            "Substituting the threshold combination **(, )nbayes cartDD  \n",
            "into the multi-layer filter model, test this multi-layer filter \n",
            "model with three separate test s et sample, sets classifiers and  \n",
            "record the accuracy rate, recall rate and accuracy.  \n",
            "4. Experiment Results \n",
            "The optimal threshold pair trained after Section 3.3 is \n",
            "**( 400.2, 1.203)nbayes cart DD   , the results are shown \n",
            "in Table 3.  \n",
            "In the multi-layer filter model, Naive Bayesian filter \n",
            "determined 308 URL, decision tree filter determined the \n",
            "1370 URL, another 322 The URL is jointly detected by three filters. This result is \u0003also in line with Table 3. The \n",
            "performance of the separate classification models can be seen in three separate models. Decision tree model is \n",
            "comparatively best performing model than Naive Bayes and \n",
            "SVM. It is also observed in Table 3 that multi-layer filter \n",
            "model performs better than all the three classifier models. \n",
            "Multi-layer filter model can let the classifier to deal with \n",
            "their own good data. Every layer of the model plays a \n",
            "beneficial role for classification of the URLs and ultimately \n",
            "improves the detection of malicious URL in terms of \n",
            "accuracy.  \n",
            "Authorized licensed use limited to: Nirma University Institute of Technology. Downloaded on September 04,2022 at 13:15:59 UTC from IEEE Xplore.  Restrictions apply. \n",
            " \n",
            "978-1-5386-1010-7/17/$31.00 ©2017 IEEE                          97  MALICIOUS URL DETECTION USING MULTI-LAYER FILTERING MODEL  \n",
            "RAJESH KUMAR, XIAOSONG ZHANG, HUSSAIN AHMAD TARIQ, RIAZ ULLAH KHAN \n",
            "School of Computer Science & Engineering, University of Electro nic Science and Technology of China  \n",
            "E-MAIL: rajakumarlohano@gmail.com, rerukhan@outlook.com\n",
            "Abstract: \n",
            "Malicious URLs are harmful to every aspect of computer \n",
            "users. Detecting of the malicious URL is very important.  \n",
            "Currently, detection of malicious webpages techniques \n",
            "includes  black-list and white-list  methodology and machine \n",
            "learning  classification algorithms are used. However, the \n",
            "black-list and  white-list technology is useless if a particular \n",
            "URL is not in  list. In this paper, we propose a multi-layer \n",
            "model for detecting  malicious  URL. The filter can directly \n",
            "determine the URL by  training the threshold of each layer \n",
            "filter when it reaches  the threshold. Otherwise, the filter \n",
            "l e a v e s  t h e  U R L  t o  n e x t  l a y e r .  W e  also used an example to \n",
            "verify that  the model can improve the  accuracy of URL \n",
            "detection. \n",
            "Keywords: \n",
            "Malicious URL; Black-list and White-list Tech nology ; \n",
            "Machine Learning; Multi-layer Filtering Model  \n",
            "1. Introduction \n",
            "In recent years, the Internet has been playing a bigger \n",
            "and bigger role in people’s work and life. Currently, we \n",
            "observed  that not every website is user-friendly and \n",
            "profitable. More and  more malicious websites began to \n",
            "appear and these malicious  websites endangering all \n",
            "aspects of the user. This can lead  the user towards \n",
            "economic losses, even some can create  confusion over the \n",
            "management of the country. Detecting and  stopping \n",
            "malicious websites has become an important control  \n",
            "measure to avoid the risk of information security [7-8] [10]. \n",
            "Generally, the only entrance to the website is URL, which \n",
            "can be malicious URL. At this level of entrance, the \n",
            "identification  o f  U R L  i s  t h e  b e s t  s o l u t i o n  t o  a v o i d  \n",
            "information loss. So the  malicious URL identification has \n",
            "a l w a y s  b e e n  a  h o t  a r e a  o f  information security. \n",
            "Spams, malicious webpages and URLs that redirect or  \n",
            "mislead the legitimate  users to malware, scams, or adult  \n",
            "content. It is perhaps correlated with the use of the internet.  \n",
            "To identify  malicious URLs, ML-based classifiers draw \n",
            "features  from webpages content (lexical, visual, etc.), URL \n",
            "lexical  features, redirect paths, host -based features, or some combinations of them. Such classifiers usually act in \n",
            "conjunction  with knowledge bases which are usually  in-\n",
            "browser URL BLs  or from web service providers. If the \n",
            "classifier is fed with  URL-based features, it is c o m m o n  \n",
            "t o  s e t  a  U R L  a g g r e g a t o r  a s  a  p e r - p r o c e s s o r  b e f o r e  \n",
            "extracting features. Mostly using  supervised learning \n",
            "paradigm, Naive  Bayes [1][13], Support V ector  Machine \n",
            "with different kernels [9], and Logistic Regression are  \n",
            "popular Machine Learning classifiers for filtering spam \n",
            "and phishing [6]. Meanwhile, GT-based learning to deal with \n",
            "active  attackers is also evaluated in  spam filtering. In this \n",
            "paper we solve this problem using multi-layer filtering \n",
            "model to do each classification which are able to handle their own relatively good data.  \n",
            "2. Multi-Layer Filter Model \n",
            "Malicious URL detection [5]\n",
            "[11-12] is a typical \n",
            "classification application scenario. The URL may be malicious URL\n",
            " o r  n o r m a l  U R L .  T h e  f i r s t  part o f  s e v e r a l  \n",
            "machine learning  classification algorithms are very useful. \n",
            "A wide range of applications have also been applied to \n",
            "malicious URL detection  scenarios. In order to give a brief \n",
            "overview to the advantages  of each classifier, this article  \n",
            "designed a malicious URL Multi-  layer detection model. \n",
            "This model is mainly composed of 4-  layer classifier, where \n",
            "these  classifiers are also called filters in  this model, so the \n",
            "model consists of 4 Layer filter composition.  \n",
            "2.1. Stratified filter \n",
            "1) Black and white list filter: The model’s first-level \n",
            "filter  is a black-and-white list filter which will be validated \n",
            "by recognizing the normal URLs and Malicious URLs. The  \n",
            "normal URL addresses are stored in the white list file, \n",
            "while  t h e  m a l i c i o u s  U R L  a d d r e s s e s  a r e  s t o r e d  i n  t h e  \n",
            "blacklist file.  To detect  the URL, we traverse the list of \n",
            "black and white to  determine whether the URL in the black \n",
            "list or in the white  list. \n",
            "2) Naive Bayesian filter: The second layer filter in \n",
            "this model is a naive Bayesian  filter that trains the model by  \n",
            "Authorized licensed use limited to: Nirma University Institute of Technology. Downloaded on September 04,2022 at 13:15:59 UTC from IEEE Xplore.  Restrictions apply.  \n",
            "98 dividing into two main steps:  \n",
            "By training the URL samples, we use two-dimensional \n",
            "arrays C1 and C2 to store the probability of each value \n",
            "of malicious website and normal website, such as formula \n",
            "(1) below:  \n",
            "11 12 1\n",
            "12, , ...\n",
            "c\n",
            ",. . .m\n",
            "i\n",
            "nn n mPP P\n",
            "PP P§·\n",
            "¨¸ ¨¸¨¸©¹\n",
            "¸\n",
            "¸\n",
            "¸\n",
            "¸\n",
            "¸\n",
            "¸\n",
            "¸         ( 1 )   \n",
            "2.2. Alpha N-Bayes threshold training \n",
            "Let m a x ( p 1/p2 ,p2/p 1 )nbayesD   (where P1 \n",
            "and P2 represent respectively what is calculated by the naive \n",
            "Bayesian formula model as malicious URL and normal URL \n",
            "probability value), so the size of nbayes can represent this classification judgment of the credibility. The nbayes describes that the URL belongs\n",
            " to one of the categories. The \n",
            "probability is much greater than the probability of belonging to another class. So, when nbayes arrives at certain threshold \n",
            "size, we can consider that URL is Naive Bayesian good data. \n",
            "If this threshold is set to too small, it may not be reached to  \n",
            "the ideal classification accuracy. If this threshold is set to too \n",
            "large, naive shellfish the data that yeast good filters will be  \n",
            "very small. This article discusses another group training data to train this threshold. The specific method is discussed \n",
            "briefly in Section 3  \n",
            "Assume that the appropriate training threshold isnbayesD . \n",
            "For the upper filter down to detect URL, put it into the traine d \n",
            "naive Bayesian model, and calculate nbayes if\n",
            "nbayes nbayesDD! , then we think the URL is a good Bayesian \n",
            "model of good data, otherwise, the URL cannot be considered as a good data for a naive Bayesian model, i.e. it cannot determine the nature of the URL, so record the classification results, and move to the next filte\n",
            "r. \n",
            "2.3. CART decision tree filter \n",
            "The model’s third-level filter is CART Decision Tree \n",
            "Filter.  \n",
            "The model is further split into two steps:  \n",
            "1) model training: The CART tree is constructed by \n",
            "training samples with URLs, and the leaf nodes are decision nodes, and store the CART tree in the file system.  \n",
            "2) cart threshold training: \n",
            "Let\n",
            "max(n/ m, m/ n)nbayesD   (n represents the number of \n",
            "malicious URL leaf nodes, m represents the number of \n",
            "normal URL leaf nodes), so the size of a cart can characterize the type of occupation that a leaf node decides. Similar to \n",
            "naive Bayesian filter, this threshold can neither be set too small nor too large. So, to train the threshold, through the \n",
            "same training data  group, the specific method is discussed in \n",
            "Section 3.  \n",
            "Ifnbayes nbayesDD! , then we think that URL is CART \n",
            "Decision tree model has a good at data, otherwise, record the classification results, and the URL is filtered to the next fil ter. \n",
            "2.4. SVM filter \n",
            "The final filter of this model is SVM filter. SVM training \n",
            "model is mainly classified models. It Derived classification function for the upper Layer filter down the URL, record classification results, combined with Naive Bayesian Filter and CART Decision Tree filtering collectively determine the \n",
            "classification of the URL.  \n",
            "3. Instance Validation \n",
            "3.1. Data sources and experimental\n",
            " environment \u0003\u0003\n",
            "The   malicious   URL    dataset    in    \n",
            "this    experiment  is downloaded from the malicious \n",
            "website lab  (Http://www.mwsl.org.cn/), the normal  U R L  \n",
            "dataset is collected from first category directory \n",
            "(http://www.dir001.com/).  10000 samples are taken from \n",
            "each dataset i.e. 10000 from  malicious URLs and 10000 \n",
            "from normal URLs. This article  uses features extraction \n",
            "and data modeling. Python language  is u s e d  a s  t h e  \n",
            "implementation programming. Windows 10  64bit as an \n",
            "operating system and Core i5 with 16GB RAM  was used \n",
            "as personal computer.  \n",
            "Table 1 feature vector extraction rules  \n",
            "No Features  \n",
            "F1 The domain names contained more  than 4 consecutive numbers  \n",
            "F2 The domain name contains special  characters (#, $, @, ~, _, -)  \n",
            "F3 Top Five domain name (com, en, net,  org, cc)  \n",
            "F4 The number of “.” in domain name  \n",
            "F5 domain name total length  \n",
            "F6 The Length of longest domain name  segment  \n",
            " \n",
            "F7 Meaningful coefficients in primary  domain names  \n",
            "3.2. Feature Selection \n",
            "Garera[2] and Gattani[3] make comparisons of \n",
            "comprehensive study regarding URL feature selection. This article mainly from the perspective of domain name cost, malicious website The creator knows that the domain name of his site has a great risk of being banned, so in the purchas e \n",
            "Authorized licensed use limited to: Nirma University Institute of Technology. Downloaded on September 04,2022 at 13:15:59 UTC from IEEE Xplore.  Restrictions apply.  \n",
            "99 When buying a domain name, you often buy cheaper or even \n",
            "free domains for cost savings Name, and this domain often has the following characteristics: 1) TLD is not the mainstream Domain name; 2) domain name with special characters; 3) domain name length is very long; 4) The main \n",
            "domain consists of meaningless letters or numbers; 5) There \n",
            "are many \".\" To confuse the domain name structure.  \n",
            "Based on the above information, this paper chooses \n",
            "seven \n",
            "features, as shown in Table 1.  \n",
            "The calculation method of F7 in Table 1 is as follows:  \n",
            "This text chooses the commonly used 5492 English \n",
            "words and 187,207 Chinese word phonetic together \n",
            "constitute a meaningful word Tree, the string of the URL ÿs \n",
            "main domain name is a meaningful list of the tree of words \n",
            "to match, if the match can be considered match these characters are there Meaning, and then use the rules of the \n",
            "characters and the total length of the characters that have ratio Meaning coefficient. For example, bookdsxihuan.com, \n",
            "because the domain name does not contain 4.  \n",
            "More than one consecutive number, so F1 is set to 0; \n",
            "due to the domain name does not contain Any of the following special words: #, $, @, ~, \u0003-, _, so F2 is set to 0; \n",
            "This example contains one of the top five domain names \"com\", so F3 is set to 1; The domain name contains a \".\" \n",
            "Therefore F4 is set to 1; F5 is the total length of the domain \n",
            "name, This example is 16; in th is case, the longest domain \n",
            "name is \"bookdsxihuan\" and its length is 12, so F6 is set \u0003to \n",
            "12; the main domain name of this example is \u0003\n",
            "\"bookdsxihuan\" with two A meaningful word \"book\", \n",
            "\"xihuan\" match \u000f\u0003meaningful length of 10, The primary \n",
            "domain has a length of 12, then it has a meaningful \n",
            "coefficient of 10/12 = 0.83. From this, the feature vector of \n",
            "this domain name is expressed as {0,0,1,1,16,12,0.83}.  \n",
            "Table 2 url data set  \n",
            "Dataset Name  Dataset Description  \n",
            "Training model dataset  8000 malicious URL, 8000 normal URL  \n",
            "Training threshold \n",
            "dataset  1000 malicious URL, 1000 normal URL  \n",
            "Testing dataset  1000 malicious URL, 1000 normal URL  \n",
            "Table 3 test results for various models  \n",
            "Model Name Accuracy \n",
            "Rate % Rexall \n",
            "Rate % Precise \n",
            "rate % \n",
            "Multi-layer filtering model 79.55 68.80 87.64 \n",
            "Simple Naive Bayes 77.30 66.40 84.91 \n",
            "Single Decision Tree 79.35 69.00 87.01 \n",
            "Single SVM 76.80 79.40 75.48 \n",
            "3.3. Training model and threshold \n",
            "In this paper, we divide the 20,000 URLs collected in \n",
            "section 3.1 into three in the experiment Part, as shown in Table 2.  \n",
            "If the URL which is to be tested can directly be judged \n",
            "as malicious or normal, so the  experimental tuning process \n",
            "does not consider black and white list filter. The blacklist filter can be considered authoritative in this model. The \n",
            "experimental \n",
            "process is as follows:  \n",
            "Perform the data on the three datasets using the feature \n",
            "extraction rules in Section 3.2  \n",
            "Train a separate machine learning with the train-model-\n",
            "set sample set Classifiers, including the naive Bayesian \n",
            "model, the CART decision tree model and SVM model  \n",
            "Build a separate machine learning model into a \n",
            "multilayer filter model, and at the very beginning give a very \n",
            "large threshold pair, so 500, 10new new\n",
            "nbayes cart DD   \n",
            "Substitute the train- threshold -set sample set into the multi-\n",
            "layer filter model and calculate the check measurement \n",
            "accuracy, and gradually reduce these two thresholds, this \n",
            "article uses the formula in the program\n",
            "1 0.8 , 1new old new old\n",
            "nbayes nbayes cart cart DD D D \u000e  \u000e  , record each \n",
            "thresholds accuracy, and finally pick the combination of the \n",
            "highest accuracy of the function **(, )nbayes cartDD  \n",
            "Substituting the threshold combination **(, )nbayes cartDD  \n",
            "into the multi-layer filter model, test this multi-layer filter \n",
            "model with three separate test s et sample, sets classifiers and  \n",
            "record the accuracy rate, recall rate and accuracy.  \n",
            "4. Experiment Results \n",
            "The optimal threshold pair trained after Section 3.3 is \n",
            "**( 400.2, 1.203)nbayes cart DD   , the results are shown \n",
            "in Table 3.  \n",
            "In the multi-layer filter model, Naive Bayesian filter \n",
            "determined 308 URL, decision tree filter determined the \n",
            "1370 URL, another 322 The URL is jointly detected by three filters. This result is \u0003also in line with Table 3. The \n",
            "performance of the separate classification models can be seen in three separate models. Decision tree model is \n",
            "comparatively best performing model than Naive Bayes and \n",
            "SVM. It is also observed in Table 3 that multi-layer filter \n",
            "model performs better than all the three classifier models. \n",
            "Multi-layer filter model can let the classifier to deal with \n",
            "their own good data. Every layer of the model plays a \n",
            "beneficial role for classification of the URLs and ultimately \n",
            "improves the detection of malicious URL in terms of \n",
            "accuracy.  \n",
            "Authorized licensed use limited to: Nirma University Institute of Technology. Downloaded on September 04,2022 at 13:15:59 UTC from IEEE Xplore.  Restrictions apply.  \n",
            "100 5. Conclusion \n",
            "In this paper, black and white list technology and \n",
            "machine learning algorithms were used and formed multi-\n",
            "layer filtering model for detection of malicious URLs. The model was trained for each machine learning algorithm i.e. \n",
            "naive Bayesian classification and decision tree classifier \n",
            "threshold and this threshold is used to refer to guide two classifiers for filtering URL. We combined the Naive Bayesian classifier, Decision Tree classifier and SVM classifiers in one multi-layer m odel to improve the malicious \n",
            "URL detection system in terms of accuracy. We observed \n",
            "from the real examples that multi-layer filtering models does \n",
            "effective detection of malicious URLs.  \n",
            "References \n",
            "[1] Hugh A. Chipman, Edward I. George, and Robert E. \n",
            "McCulloch . “Bayesian CART Model Search.” Journal \n",
            "of the American Statistical Association, Vol. 93(443), \n",
            "pp 935 –948, September 1998. \n",
            "[2] \n",
            "Sujata Garera, Niels Provos, Monica Chew, and Aviel \n",
            "D. Rubin. “A framework for detection and \n",
            "measurement of phishing attacks.” In Pro ceedings of \n",
            "the 2007 ACM workshop on Recurring malicious code \n",
            "- WORM ’07, page 1, 2007.  \n",
            "[3] Abhishek Gattani, AnHai Doan, Digvijay S. Lamba, \n",
            "Nikesh Garera, Mitul Tiwari, Xiaoyong Chai, Sanjib \n",
            "Das, Sri Subramaniam, Anand Rajaraman, and Venky \n",
            "Harinarayan. “Entit y extraction, linking, classifica- tion, \n",
            "and tagging for social media.” Proceedings of the \n",
            "VLDB Endowment, Vol. 6(11), pp 1126 –1137, August \n",
            "2013. \n",
            "[4] David D. Lewis. Naive (Bayes) at forty: The independence assumption in  information retrieval. \n",
            "pages 4 –15. 1998. \n",
            "[5] Justin Ma, Lawrence K. Saul, Stefan Savage, and \n",
            "Geoffrey M. Voelker. “Learning to detect malicious \n",
            "URLs.”  ACM Transactions on Intelligent Systems and Technology, Vol. 2(3), pp 1 –24, April 2011. \n",
            "[6] \n",
            "Fadi Thabtah Maher Aburrous, M.A.Hossain, Keshav \n",
            "Dahal. “In telligent phishing detection system for e-\n",
            "banking using fuzzy data mining.” Expert Systems with \n",
            "Applications, Vol. 37(12), pp 7913 –7921, Dec 2010. \n",
            "[7] Ankush Meshram and Christian Haas. “Anomaly \n",
            "Detection in Industrial. Networks using Machine Learning: A Roadm ap.” In Machine Learning for Cyber \n",
            "Physical Systems, pages 65 –72. Springer Berlin \n",
            "Heidelberg, Berlin , Heidelberg, 2017. \n",
            "[8] \n",
            "Xuequn Wang Nik Thompson,Tanya Jane McGill.  \n",
            "“Security begins at home: Determinants of home computer and mobile device security behavior .” \n",
            "Computers & Security, Vol. 70, pp 376 –391, Sep 2017. \n",
            "[9] Dan Steinberg and Phillip Colla. “CART: Classification and Regression Trees.” The Top Ten Algorithms in Data Mining, pp 179 –201, 2009. \n",
            "[10] D. Teal. “Information security techniques including detection, interdiction and/or mitigation of memory \n",
            "injection attacks,” Google patents. Oct 2013.  \n",
            "[11] Kurt Thomas, Chris Grier, Justin Ma, Vern Paxson, and Dawn Song. “Design and Evaluation of a Real -Time \n",
            "URL Spam Filtering Service.” In 2011 IEEE \n",
            "Symposium on Security and Privacy, pp 447 –462. May \n",
            "2011. \n",
            "[12] Sean Whalen, Nathaniel Boggs, and Salvatore J. Stolfo. \n",
            "“Model Aggregation for Distributed Content Anomaly Detection.” In Proceedings of the 2014 Workshop on Artificial Intelligent and Security Workshop - \n",
            "AISec ’14, pp 61– 71, New York, USA, 2014. ACM \n",
            "Press. \n",
            "[13] Ying Yang and Geoffrey I. Webb. “Discretization for \n",
            "Naive-Bayes learning: managing a discretization bias and variance.” Machine Learning, Vol. 74(1), pp 39– 74, \n",
            "Jan 2009. \n",
            "Authorized licensed use limited to: Nirma University Institute of Technology. Downloaded on September 04,2022 at 13:15:59 UTC from IEEE Xplore.  Restrictions apply. \n"
          ]
        }
      ],
      "source": [
        "import PyPDF2\n",
        "import io\n",
        "import random\n",
        "\n",
        "pdfReader = PyPDF2.PdfReader(\"/content/Malicious_URL_detection_using_multi-layer_filtering_model.pdf\")\n",
        "\n",
        "num_pages = len(pdfReader.pages)\n",
        "count = 0\n",
        "text = \"\"\n",
        "  #The while loop will read each page.\n",
        "while count < num_pages:\n",
        "  pageObj = pdfReader.pages[count]\n",
        "  count +=1\n",
        "  text += pageObj.extract_text()\n",
        "\n",
        "import re\n",
        "\n",
        "words = re.findall(r\"[\\s\\w]\", text)\n",
        "words = \"\".join(words)\n",
        "words = words.lower()\n",
        "for i in range(len(words)):\n",
        "  if words[i].isdigit():\n",
        "    words = words[:i]+' '+words[i+1:]\n",
        "\n",
        "words = words.split()\n",
        "for i in words:\n",
        "  if len(i)<3:\n",
        "    words.remove(i)\n",
        "\n",
        "stop = nltk.corpus.stopwords.words('english')\n",
        "for i in stop:\n",
        "  while i in words:\n",
        "    words.remove(i)\n",
        "\n",
        "words = ' '.join(words)\n",
        "\n",
        "x = words.split()\n",
        "tokens = []\n",
        "for i in x:\n",
        "  if len(i)>3:\n",
        "    tokens.append(i)\n",
        "\n",
        "tags = nltk.pos_tag(tokens)\n",
        "\n",
        "from autocorrect import Speller\n",
        "spell = Speller()\n",
        "for i in range(len(tokens)):\n",
        "  if tags[i][1] not in ['NN', 'JJ', 'NNP']:\n",
        "    tokens[i] = spell(tokens[i])\n",
        "\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "iU-YQObfU8C8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iU-YQObfU8C8",
        "outputId": "d5085149-797a-4878-9948-20852b155b62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: autocorrect in /usr/local/lib/python3.8/dist-packages (2.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install autocorrect"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ng8EfqbvSDY",
      "metadata": {
        "id": "9ng8EfqbvSDY"
      },
      "source": [
        "# Plag Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "13v6Uv7G2x1d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13v6Uv7G2x1d",
        "outputId": "0cb4d7e7-b58e-4841-ab39-adad82198d87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.8/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wikipedia) (2.25.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (4.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "cNYGB8IhRiwT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNYGB8IhRiwT",
        "outputId": "685b1048-fb52-4862-e4d6-88f6fccbfe99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['institute', 'electrical', 'electronics', 'engineers', 'ieee', 'professional', 'association', 'electronics', 'engineering', 'electrical', 'engineering', 'related', 'disciplines', 'corporate', 'office', 'york', 'city', 'operations', 'center', 'piscataway', 'jersey', 'mission', 'ieee', 'advancing', 'technology', 'benefit', 'humanity', 'ieee', 'formed', 'amalgamation', 'american', 'institute', 'electrical', 'engineers', 'institute', 'radio', 'engineers', 'expansion', 'scope', 'many', 'related', 'fields', 'simply', 'referred', 'letters', 'ieee', 'pronounced', 'itriplee', 'except', 'legal', 'business', 'documents', 'worlds', 'largest', 'association', 'technical', 'professionals', 'members', 'countries', 'around', 'world', 'objectives', 'educational', 'technical', 'advancement', 'electrical', 'electronics', 'engineering', 'telecommunications', 'computer', 'engineering', 'similar', 'disciplines', 'history', 'origins', 'ieee', 'traces', 'founding', 'american', 'institute', 'electrical', 'engineers', 'rival', 'institute', 'radio', 'engineers', 'formed', 'although', 'aiee', 'initially', 'larger', 'attracted', 'students', 'larger', 'aiee', 'merged', 'ieee', 'headquarters', 'york', 'city', 'park', 'business', 'done', 'ieee', 'operations', 'center', 'piscataway', 'first', 'occupied', 'growth', 'american', 'institute', 'electrical', 'engineers', 'aiee', 'institute', 'radio', 'engineers', 'merged', 'create', 'institute', 'electrical', 'electronics', 'engineers', 'ieee', 'january', 'time', 'combined', 'group', 'members', 'united', 'states', 'members', 'outside', 'usthe', 'australian', 'section', 'ieee', 'existed', 'date', 'split', 'state', 'territorybased', 'sectionsas', 'ieee', 'members', 'countries', 'based', 'membership', 'longer', 'constituting', 'majority', 'controversies', 'huawei', 'ieee', 'restricted', 'huawei', 'employees', 'peer', 'reviewing', 'papers', 'handling', 'papers', 'editors', 'severe', 'legal', 'implications', 'government', 'sanctions', 'huawei', 'members', 'standardsetting', 'body', 'huawei', 'employees', 'could', 'continue', 'exercise', 'voting', 'rights', 'attend', 'standards', 'development', 'meetings', 'submit', 'proposals', 'comment', 'public', 'discussions', 'standards', 'sparked', 'outrage', 'among', 'chinese', 'scientists', 'social', 'media', 'professors', 'china', 'decided', 'cancel', 'membershipson', 'june', 'ieee', 'lifted', 'restrictions', 'huaweis', 'editorial', 'peer', 'review', 'activities', 'receiving', 'clearance', 'united', 'states', 'government', 'position', 'russiaukraine', 'conflict', 'february', 'chair', 'ieee', 'ukraine', 'section', 'ievgen', 'pichkalov', 'publicly', 'appealed', 'ieee', 'members', 'freeze', 'ieee', 'activities', 'membership', 'russia', 'requested', 'public', 'reaction', 'strict', 'disapproval', 'russias', 'aggression', 'ieee', 'ieee', 'region', 'march', 'article', 'form', 'interview', 'ieee', 'russia', 'siberia', 'senior', 'member', 'roman', 'gorbunov', 'titled', 'russian', 'perspective', 'ukraine', 'published', 'ieee', 'spectrum', 'demonstrate', 'plurality', 'views', 'among', 'ieee', 'members', 'views', 'odds', 'international', 'reporting', 'ukraine', 'march', 'activist', 'anna', 'rohrbach', 'created', 'open', 'letter', 'ieee', 'attempt', 'directly', 'address', 'article', 'stating', 'article', 'used', 'common', 'narratives', 'russian', 'propaganda', 'russian', 'invasion', 'ukraine', 'requesting', 'ieee', 'spectrum', 'acknowledge', 'unwittingly', 'published', 'piece', 'furthering', 'misinformation', 'russian', 'propaganda', 'days', 'later', 'note', 'editors', 'added', 'april', 'apology', 'providing', 'adequate', 'context', 'time', 'publication', 'though', 'editors', 'revise', 'original', 'article', 'publications', 'ieee', 'claims', 'produce', 'worlds', 'literature', 'electrical', 'electronics', 'computer', 'engineering', 'fields', 'publishing', 'approximately', 'peerreviewed', 'journals', 'magazines', 'ieee', 'publishes', 'conference', 'proceedings', 'every', 'year', 'published', 'content', 'journals', 'well', 'content', 'several', 'hundred', 'annual', 'conferences', 'sponsored', 'ieee', 'available', 'ieee', 'electronic', 'library', 'available', 'ieee', 'xplore', 'platform', 'subscriptionbased', 'access', 'individual', 'publication', 'purchasesin', 'addition', 'journals', 'conference', 'proceedings', 'ieee', 'also', 'publishes', 'tutorials', 'standards', 'produced', 'standardization', 'committees', 'organization', 'also', 'ieee', 'paper', 'format', 'educational', 'activities', 'ieee', 'provides', 'learning', 'opportunities', 'within', 'engineering', 'sciences', 'research', 'technology', 'ieee', 'offers', 'educational', 'opportunities', 'ieee', 'elearning', 'library', 'education', 'partners', 'program', 'standards', 'education', 'continuing', 'education', 'units', 'ceusieee', 'elearning', 'library', 'collection', 'online', 'educational', 'courses', 'designed', 'selfpaced', 'learning', 'education', 'partners', 'exclusive', 'ieee', 'members', 'offers', 'online', 'degree', 'programs', 'certifications', 'courses', 'discount', 'standards', 'education', 'website', 'explains', 'standards', 'importance', 'developing', 'using', 'site', 'includes', 'tutorial', 'modules', 'case', 'illustrations', 'introduce', 'history', 'standards', 'basic', 'terminology', 'applications', 'impact', 'products', 'well', 'news', 'related', 'standards', 'book', 'reviews', 'links', 'sites', 'contain', 'information', 'standards', 'currently', 'forty', 'states', 'united', 'states', 'require', 'professional', 'development', 'hours', 'maintain', 'professional', 'engineering', 'license', 'encouraging', 'engineers', 'seek', 'continuing', 'education', 'units', 'ceus', 'participation', 'continuing', 'education', 'programs', 'ceus', 'readily', 'translate', 'professional', 'development', 'hours', 'pdhs', 'equivalent', 'pdhs', 'countries', 'outside', 'united', 'states', 'south', 'africa', 'similarly', 'require', 'continuing', 'professional', 'development', 'credits', 'anticipated', 'ieee', 'expert', 'courses', 'feature', 'listing', 'south', 'africa', 'ieee', 'also', 'sponsors', 'website', 'designed', 'help', 'young', 'people', 'better', 'understand', 'engineering', 'website', 'allows', 'students', 'search', 'accredited', 'engineering', 'degree', 'programs', 'canada', 'united', 'states', 'student', 'activities', 'committee', 'ieee', 'facilitates', 'partnership', 'student', 'activities', 'ieee', 'entities', 'technical', 'societies', 'various', 'technical', 'areas', 'addressed', 'ieees', 'societies', 'focused', 'certain', 'knowledge', 'area', 'provide', 'specialized', 'publications', 'conferences', 'business', 'networking', 'sometimes', 'services', 'ieee', 'computer', 'society', 'ieee', 'computer', 'society', 'largest', 'among', 'ieee', 'societies', 'membership', 'flagship', 'publication', 'included', 'part', 'membership', 'computer', 'members', 'members', 'sign', 'free', 'computing', 'edge', 'magazine', 'digest', 'content', 'previously', 'published', 'several', 'ieee', 'computer', 'society', 'magazines', 'ieee', 'global', 'history', 'network', 'september', 'ieee', 'history', 'committee', 'founded', 'ieee', 'global', 'history', 'network', 'redirects', 'engineering', 'technology', 'history', 'wiki', 'ieee', 'foundation', 'ieee', 'foundation', 'charitable', 'foundation', 'established', 'support', 'promote', 'technology', 'education', 'innovation', 'excellence', 'incorporated', 'separately', 'ieee', 'although', 'close', 'relationship', 'members', 'board', 'directors', 'foundation', 'required', 'active', 'members', 'ieee', 'third', 'must', 'current', 'former', 'members', 'ieee', 'board', 'directors', 'initially', 'role', 'ieee', 'foundation', 'accept', 'administer', 'donations', 'ieee', 'awards', 'program', 'donations', 'increased', 'beyond', 'necessary', 'purpose', 'scope', 'broadened', 'addition', 'soliciting', 'administering', 'unrestricted', 'funds', 'foundation', 'also', 'administers', 'donordesignated', 'funds', 'supporting', 'particular', 'educational', 'humanitarian', 'historical', 'preservation', 'peer', 'recognition', 'programs', 'ieee', 'foundations', 'total', 'assets', 'nearly', 'million', 'split', 'equally', 'unrestricted', 'donordesignated', 'funds', 'also', 'certified', 'software', 'development', 'professional', 'csdp', 'program', 'ieee', 'computer', 'society', 'glossary', 'electrical', 'electronics', 'engineering', 'engineering', 'technology', 'history', 'wiki', 'kappa', 'ieee', 'honor', 'society', 'ieee', 'standards', 'association', 'institution', 'engineering', 'technology', 'international', 'electrotechnical', 'commission', 'list', 'ieee', 'awards', 'list', 'ieee', 'conferences', 'list', 'ieee', 'fellows', 'references', 'external', 'links', 'media', 'related', 'institute', 'electrical', 'electronics', 'engineers', 'wikimedia', 'commons', 'official', 'website', 'ieee', 'xplore', 'research', 'database', 'online', 'digital', 'library', 'archive', 'ieee', 'history', 'center']\n"
          ]
        }
      ],
      "source": [
        "text_1 = wiki_pages[0].content\n",
        "words = re.findall(r\"[\\s\\w]\", text_1)\n",
        "words = \"\".join(words)\n",
        "words = words.lower()\n",
        "for i in range(len(words)):\n",
        "  if words[i].isdigit():\n",
        "    words = words[:i]+' '+words[i+1:]\n",
        "\n",
        "words = words.split()\n",
        "for i in words:\n",
        "  if len(i)<3:\n",
        "    words.remove(i)\n",
        "\n",
        "stop = nltk.corpus.stopwords.words('english')\n",
        "for i in stop:\n",
        "  while i in words:\n",
        "    words.remove(i)\n",
        "\n",
        "words = ' '.join(words)\n",
        "\n",
        "x = words.split()\n",
        "wiki_tokens = []\n",
        "for i in x:\n",
        "  if len(i)>3:\n",
        "    wiki_tokens.append(i)\n",
        "print(wiki_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "RctQSTI5BX96",
      "metadata": {
        "id": "RctQSTI5BX96"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import wikipedia\n",
        "\n",
        "wiki_pages = []\n",
        "for i in tokens:\n",
        "  try:\n",
        "    wiki_pages.append(wikipedia.page(i))\n",
        "  except:\n",
        "    continue\n",
        "print(wiki_pages)\n",
        "\n",
        "model = gensim.models.doc2vec.Doc2Vec(vector_size=30, min_count=2, epochs=80)\n",
        "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(text)]\n",
        "model.build_vocab(tagged_data)\n",
        "model.train(tagged_data, total_examples=model.corpus_count, epochs=80)\n",
        "\n",
        "data = word_tokenize(' '.join(tokens))\n",
        "token_vector = model.infer_vector(data)\n",
        "\n",
        "wiki_dict = {}\n",
        "for i in wiki_pages:\n",
        "  data = word_tokenize(i.content)\n",
        "  wiki_dict[i.url] = model.infer_vector(data)\n",
        "\n",
        "def cosine_sim(a, b):\n",
        "  sum = 0\n",
        "  sum_a = 0\n",
        "  sum_b = 0\n",
        "  for i in range(len(a)):\n",
        "    sum_a += a[i]**2\n",
        "    sum_b += b[i]**2\n",
        "    sum += (a[i]*b[i])\n",
        "\n",
        "  return sum/((sum_a**(1/2))*(sum_b**(1/2)))\n",
        "\n",
        "cos_vals = {}\n",
        "for i in wiki_pages:\n",
        "  cos_vals[i.url] = cosine_sim(token_vector, wiki_dict[i.url])\n",
        "\n",
        "print('The descending order of similarity index with different websites is:')\n",
        "ranked = sorted(cos_vals, key=lambda x:cos_vals[x], reverse=True)\n",
        "for i in ranked:\n",
        "  if cos_vals[i]>0.3:\n",
        "    print(i, cos_vals[i])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
